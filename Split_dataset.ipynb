{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting a dataset for machine learning study\n",
    "\n",
    "<p style=\"text-align: justify\">In this notebook, we will see how to perform a split of a set of data when starting data analysis with Python. A procedural approach which is more comprehensive for beginners is used, without using the <a href=\"https://pandas.pydata.org/\">Pandas</a> and <a href=\"https://www.scipy.org/\">SciPy</a> packages, that are regularly used for data analysis and data scraping. This choice is done for better understanding of the different functions that are already implemented in these libraries, and to comprehend from scratch the programming behind. The different lines of code are well commented for the reader to understand. This notebook follows the one concerning <a href=\"https://github.com/DavidCico/Simple-functions-for-starting-machine-learning-with-Python/blob/master/Open_conversion_data.ipynb\"The> data loading, and pre-processing operations</a>. The first step in the current notebook is to import the different built-in modules in Python that are required to compute our code.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The <i>random.seed()</i> built-in function is usually employed to generate the same pseudo-random numbers, based on the seed value. This is useful when comparing different algorithms: indeed, as machine learning problems are random and stochastic, it is important to make a comparison of the models for the same random numbers, in order to remove some uncertainty. The algorithms can further be compared with different seed numbers, but <b>using the same seed is primordial</b> at each step of the comparison.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The <i>random.randrange()</i> built-in function returns a randomly selected element from a specified range. The <i>random.randint()</i> fucntion returns a random integer from a specified range. For more information on the random module, the reader can go <a href=\"https://docs.python.org/2/library/random.html\"> here</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed \n",
    "from random import randrange\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split\n",
    "\n",
    "<p style=\"text-align: justify\"> When it comes to machine learning problems, it is necessary to define and assess models based on the amount of accessible data. Whether it is a classification or regression problem, this requires that the models are first trained and then tested on different datasets. Thus, the main dataset is usually split in to 2 distinct datasets, with different purposes:</p>\n",
    "\n",
    "<ol>\n",
    "    <li>A <b>training dataset</b>, which is the subset to train the model.</li>\n",
    "    <li>A <b>testing dataset</b>, to test the trained model.</li>\n",
    "</ol>\n",
    "\n",
    "<img src=\"https://developers.google.com/machine-learning/crash-course/images/PartitionTwoSets.svg\" width = 700>\n",
    "\n",
    "<p style=\"text-align: justify\">The rows assigned to each dataset are randomly selected. This is an attempt to ensure that the training and evaluation of a model is objective. If multiple algorithms are compared or multiple configurations of the same algorithm are compared, the same train and test split of the dataset should be used. This is to ensure that the comparison of performance is consistent or apples-to-apples. We can achieve this by seeding the random number generator the same way before splitting the data, or by holding the same split of the dataset for use by multiple algorithms. We can implement the train and test split of a dataset in a single function.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Below is the function <i>train_test_split()</i> that implements this procedure. The function takes both the \"dataset\", and a floating number \"split\" as arguments. The split number (between 0 and 1) indicates in which proportion, we want to split the dataset. There is no such \"correct\" value to split the dataset as it may depend on your dataset size and problem but a <b>70% train / 30% test</b> is usually a right way to start evaluating algorithms. The test set should follow these recommendations:</p>\n",
    "<ul>\n",
    "<li>Be large enough to yield statistically meaningful results.</li>\n",
    "<li>Be representative of the data set as a whole.  In other words, don&#39;t pick\n",
    "a test set with different characteristics than the training set.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">In a function implemented below, a training set (list) is first initalized, with its size defined by the \"train_size\" variable. A copy of the full dataset 'dataset_copy' is made, as the operations will be carried out on this copy, to preserve the regular dataset. The train set is then populated by random rows of the copy dataset, which are removed from the latter, until the train dataset is of length \"train_size\". The two datasets, are then returned and the \"dataset_copy\" is now the testing dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into a train and test set\n",
    "def train_test_split(dataset, split):\n",
    "    train = list()\n",
    "    train_size = split * len(dataset) # define the length of training dataset\n",
    "    dataset_copy = list(dataset) # creates copy of the dataset\n",
    "    while len(train) < train_size: \n",
    "        index = randrange(len(dataset_copy)) # look for a random row index in the copy_dataset (testing_dataset)\n",
    "        train.append(dataset_copy.pop(index)) # append this row to the training dataset, and remove it from the testing one\n",
    "    return train, dataset_copy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The example below shows the splitting for a simple dataset. The training and testing dataset returned are always the same as the seed number is fixed. As explained at the start of the notebbok, it is critical to use the same seed number when splitting a dataset, in order to compare different algorithms with the same subsets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  [[2], [4], [5], [15], [9], [17], [19], [16], [8], [10], [20], [7], [1], [13]]\n",
      "Testing set:  [[3], [6], [11], [12], [14], [18]]\n"
     ]
    }
   ],
   "source": [
    "# Test train/test split\n",
    "\n",
    "seed(2)\n",
    "split = 0.7 # split defined as 70/30 for train/test\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10],[11],[12],[13],[14],[15],[16],[17],[18],[19],[20]]\n",
    "train, test = train_test_split(dataset,split)\n",
    "print(\"Training set: \", train)\n",
    "print(\"Testing set: \", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *k*- fold cross-validation\n",
    "\n",
    "<p style=\"text-align: justify\">One major drawback of using a train/test split approach is that it can be quite noisy and not good for the performance assessment of the algorithm. The <i>k</i>-fold cross-validation method (also called just cross-validation) is a resampling method that provides a more accurate estimate of algorithm performance.</p>\n",
    "\n",
    "<p style=\"text-align: justify\"> As there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. By reducing the training data, we risk losing important patterns/ trends in the dataset, which in turn increases the error induced by bias. So, what we require is a method that provides ample data for training the model and also leaves ample data for validation. <i>k</i>-fold cross-validation does exactly that.\n",
    "\n",
    "<p style=\"text-align: justify\">In <i>k</i>-fold cross-validation, the data is divided into <i>k</i> subsets. The holdout method (keeping some data as testing set) is repeated <i>k</i> times, such that each time, one of the <i>k</i> subsets is used as the test set/ validation set and the other <i>k-1</i> subsets are put together to form a training set. The error estimation is averaged over all <i>k</i> trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set <i>k-1</i> times. This significantly reduces bias, as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method. As a general rule and empirical evidence, <i>k</i> = 5 or 10 is generally preferred, but nothing is fixed and it can take any value.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The function <i>cross_validation_split()</i> below takes in argument both the \"dataset\" and \"n_folds\", which is the number of <i>k</i> folds we want to use for the cross-validation method. A \"dataset_split\" list is first initialized and a copy of the dataset is made, where the different operations are made. The \"dataset_split\" will contain the different folds used for cross-validation and is returned by the function. For the folds definition, it is similar to the <i>train_test_split()</i> function defined above, with some loop over the number of folds. Indeed, the \"fold_size\" is first defined by dividing the total number of rows of the dataset, by \"n_folds\". Considering the first fold, it is populated by random rows of the copy dataset until it reaches the \"fold_size\". These rows are also removed from the copy dataset (using .pop(index)), and the fold is added to the \"dataset_split\" list. The loop goes on with the same procedure until all folds are defined. At the end, the copy dataset is basically empty as all of its rows have been shuffled and saved into the folds.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into $k$ folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The example below shows the cross-validation split on a generated dataset. The dataset, is made of 10 rows of 2 columns in which the <i>random.randint()</i> function returns a random integer between 1 and 20. The seed is again fixed to show the pseudo-random nature of the split, which is necessary when comparing different algorithms (on same subsets.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset : [[5, 19], [3, 9], [4, 16], [15, 16], [13, 7], [4, 16], [1, 13], [14, 20], [1, 15], [9, 8]]\n",
      "Folds list : [[[9, 8], [3, 9]], [[1, 13], [5, 19]], [[4, 16], [15, 16]], [[13, 7], [14, 20]], [[4, 16], [1, 15]]]\n",
      "Fist fold : [[9, 8], [3, 9]]\n"
     ]
    }
   ],
   "source": [
    "# Test cross-validation split\n",
    "seed(1) # fixed seed number\n",
    "dataset = [[randint(1,20) for _ in range(2)] for _ in range(10)] # nested list of 2 colums with random integer, repeated for 10 rows\n",
    "n_folds = 5 # number of folds\n",
    "folds = cross_validation_split(dataset, n_folds)\n",
    "\n",
    "print(\"Full dataset :\", dataset)\n",
    "print(\"Folds list :\", folds)\n",
    "print(\"Fist fold :\", folds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of resampling method\n",
    "\n",
    "<p style=\"text-align: justify\">The best way for assessing the performance of machine learning algorithms on new data is <i>k</i>-fold cross-validation. When well configured, <i>k</i>-fold cross-validation gives a robust estimate of performance compared to other methods such as the train and test split. The major drawback of cross-validation is basically its expense, as it is quite time consuming, depending on the length of the dataset and also the different models that need to be compared or trained. There is a compromise, between accuracy and speed.</p>\n",
    "\n",
    "<p style=\"text-align: justify\"> This is why the train/test split resampling method is usually preferred despite its drawbacks. It provides a quick estimation of the model capabilities by just training the model on a single dataset. When considering different models for comparison, and such as artificial neuron networks (ANN) which can be quite long to train, the train/test split is the only solution. Furthermore despite the drawback of adding noise in the background, it must be reminded that this issue can be reduced when the dataset becomes large. Large datasets are those in the hundreds of thousands or millions of records, big enough that splitting it in half results in two datasets that have nearly equivalent statistical properties. In such cases, there may be little need to use <i>k</i>-fold cross-validation as an evaluation of the algorithm and a train and test split may be just as reliable.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "Potential extensions for the reader willing to explore from scratch more splitting method:\n",
    "\n",
    "<ul>\n",
    "<li><p style=\"text-align: justify\"><b>Repeated Train and Test or Monte-Carlo cross-validation</b>: In this type of cross-validation, the split ratio between training and testing dataset is fixed, and the data is randomly shuffled at each split. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over <i>k</i>-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (folds). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap.</p></li>\n",
    "<br>\n",
    "    <li><p style=\"text-align: justify\"><b>LOOCV or Leave One Out Cross-Validation</b>: This is a form of <i>k</i>-fold cross-validation where the value of <i>k</i> is fixed at 1. LOO cross-validation requires less computation time than LpO cross-validation because there are only $C_{1}^{n}=n$ passes rather than $C_{k}^{n}$. However, $n$ passes may still require quite a large computation time, in which case other approaches such as <i>k</i>-fold cross validation may be more appropriate. </p></li>\n",
    "<br>\n",
    "    <li><p style=\"text-align: justify\"><b>Stratification</b>: In classification problems, this is where the balance of class values in each group is forced to match the original dataset.</p></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
