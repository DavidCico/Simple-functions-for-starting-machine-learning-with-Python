{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from a csv file, and pre-processing data for further analysis\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">In this notebook, we will see how to perform simple and essential tasks when starting data analysis with Python. A procedural approach which is more comprehensive for beginners is used, without using the <a href=\"https://pandas.pydata.org/\">Pandas</a> and <a href=\"https://www.scipy.org/\">SciPy</a> packages that are regularly used for data analysis and data scraping. This choice is done for better understanding of the different functions that are already implemented in these libraries, and to comprehend from scratch the programming behind. The different lines of code are well commented for the reader to understand. The first step in the current notebook is to import the different built-in modules in Python that are required to compute our code. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # csv for reading the csv files\n",
    "from math import sqrt # square root function from math module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">The data could be obtained by <a href=\"https://en.wikipedia.org/wiki/Data_scraping\">data scrapping</a> from a website for instance or via an <a href=\"https://en.wikipedia.org/wiki/Application_programming_interface\">Application Programming Interface (API)</a> . \n",
    "\n",
    "Here instead, we consider loading the data from a CSV file as it is one the most common data-exchange format. A <a href=\"https://en.wikipedia.org/wiki/Comma-separated_values\">comma separated values</a> (CSV) file contains different values separated by a delimiter, which acts as a database table or an intermediate form of a database table. In other words, a CSV file file is a set of database rows and columns stored in a text file such that the rows are separated by a new line while the columns are separated by a semicolon or a comma. A CSV file is primarily used to transport data between two databases of different formats through a computer program.\n",
    "\n",
    "The function <i>load_csv()</i> below, scrapes the data from a CSV file by reading the different rows of the file, and by storing the rows one by one in a list.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()  # creates a list where the row will be stored\n",
    "    with open(filename, 'r') as file: # open the file in reading mode \n",
    "        csv_reader = csv.reader(file) # csv.reader built-in function\n",
    "        for row in csv_reader: # loop on the rows of the file\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row) # adding row to the dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">The data acquired from the function above can be of different types. The data can be made of strings of characters or floating numbers for instance. When reading from a CSV file, the data is parsed as a string of character. However, in order to perform computational operations and analysis on the data, this requires that all string of numbers are transformed into floating numbers.</div>   \n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">The function <i>str_column_to_float()</i> below implements this operation for a column of the dataset. The function can then later be looped on the desired columns to turn strings into floating numbers. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip()) # strip() function strips all characters from the beginning and the end of the string (default whitespace characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">The function <i>str_column_to_float()</i> will usually be used to transform all the features of the dataset into floating numbers to perform different operations on them. However, when considering dataset for classification especially, different classes may be a string of characters as well. For instance, let's consider that in a group of person, these people are classified by age and we have 2 classes 'Below 30' and 'Above 30'. Operations won't be performed on these classes as they will just be used for comparison with predictions, for instance in a machine learning classification problem.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">However, transforming the data from this column into integers, can ease the classification process. For example, the string 'Below 30' is replaced by the number 0 and 'Above 30' by 1. In this way, predictions from the features are easier to compare with the classification values.</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: justify\">The function <i>str_column_to_int()</i> below allows this conversion operation for a particular column. First, a list ('class_values') containing the different rows of the column is created. From this list, 'unique' is created which is a set looking at all unique values of the list 'class_values'. A dictionary ('lookup') is then initialised to match integer values to the string characters. The string characters can now be referenced with their integer values, which will be usually preferred for data analysis.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset] # comprehension list with data of the considered column stored\n",
    "    unique = set(class_values) # set where all unique values are stored\n",
    "    lookup = dict() #\n",
    "    for i, value in enumerate(unique): # enumerate function (loop + automatic counter of values)\n",
    "        lookup[value] = i # value is the key of dictionary (the string characters) and i is the number affiliated\n",
    "    for row in dataset:  # modifying column in dataset\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file iris.csv with 150 rows and 5 columns\n",
      "First row of the dataset:  ['5.1', '3.5', '1.4', '0.2', 'Iris-setosa']\n",
      "--------------------------------------\n",
      "First row of modified dataset:  [5.1, 3.5, 1.4, 0.2, 2]\n",
      "{'Iris-virginica': 0, 'Iris-versicolor': 1, 'Iris-setosa': 2}\n"
     ]
    }
   ],
   "source": [
    "# Load iris dataset\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset), len(dataset[0])))\n",
    "print('First row of the dataset: ', dataset[0]) # print first line of the dataset\n",
    "print('--------------------------------------')\n",
    "\n",
    "# convert string columns to float \n",
    "for i in range(len(dataset[0])-1): # loop on all columns\n",
    "    str_column_to_float(dataset, i)\n",
    "# convert class column to int\n",
    "lookup = str_column_to_int(dataset, 4)\n",
    "\n",
    "print('First row of modified dataset: ', dataset[0]) # print first line of updated dataset\n",
    "print(lookup) # print lookup dictionary containing the classification of the species and their corresponding number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Normalize Data ###########\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        colvalues = [row[i] for row in dataset]\n",
    "        min_value = min(colvalues) \n",
    "        max_value = max(colvalues)\n",
    "        minmax.append([min_value, max_value])\n",
    "    return minmax\n",
    "\n",
    "# Normalize the dataset except last row for classification values\n",
    "def Normalize_Dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of normalized dataset:  [0.22222222222222202, 0.6249999999999999, 0.06779661016949144, 0.041666666666666644, -1.2206555615733707]\n"
     ]
    }
   ],
   "source": [
    "# Calculate min and max for each column\n",
    "minmax = dataset_minmax(dataset)\n",
    "# Normalize columns\n",
    "Normalize_Dataset(dataset, minmax)\n",
    "print('First row of normalized dataset: ', dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standardize Data ######\n",
    "\n",
    "# Calculate column means\n",
    "def column_means(dataset):\n",
    "    means = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        means[i] = sum(col_values) / float(len(dataset))\n",
    "    return means\n",
    "\n",
    "# Calculate column standard deviations\n",
    "def column_stdevs(dataset, means):\n",
    "    stdevs = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        variance = [pow(row[i]-means[i], 2) for row in dataset]\n",
    "        stdevs[i] = sum(variance)\n",
    "        stdevs = [sqrt(x/(float(len(dataset)-1))) for x in stdevs]\n",
    "    return stdevs\n",
    "\n",
    "# Standardize the dataset\n",
    "def Standardize_Dataset(dataset, means, stdevs):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - means[i]) / stdevs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.179297646015436e+22, 98315463418529.08, -4160058.767277357, -194.9783686428172, -1.2206555615733707]\n"
     ]
    }
   ],
   "source": [
    "# Estimate mean and standard deviation\n",
    "means = column_means(dataset)\n",
    "stdevs = column_stdevs(dataset, means)\n",
    "# standardize dataset\n",
    "Standardize_Dataset(dataset, means, stdevs)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
