{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms performance assessment\n",
    "\n",
    "<p style=\"text-align: justify\">After you make predictions, you need to know if they are any good. There are standard measures\n",
    "that we can use to summarize how good a set of predictions actually is. Knowing how good a set of predictions is allows you to make estimates about the skill of a given machine learning model of your problem. This notebook shows the implementation of such evaluation metrics for both classification and regression procedures. A procedural approach which is more comprehensive for beginners is used, without using the <a href=\"https://pandas.pydata.org/\">Pandas</a> and <a href=\"https://www.scipy.org/\">SciPy</a> packages, that are regularly used for data analysis and data scraping. This choice is done for better understanding of the different functions that are already implemented in these libraries, and to comprehend from scratch the programming behind. The different lines of code are well commented for the reader to understand. This notebook follows the one concerning <a href=\"https://github.com/DavidCico/Simple-functions-for-starting-machine-learning-with-Python/blob/master/Open_conversion_data.ipynb\"The> data loading, and pre-processing operations</a> and <a href=\"https://github.com/DavidCico/Simple-functions-for-starting-machine-learning-with-Python/blob/master/Split_dataset.ipynb\">data split</a>.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The first step in the current notebook is to import the different built-in modules in Python that are required to compute our code. Only the <em>\"sqrt\"</em> function is necessary.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">You must estimate the quality of a set of predictions when training a machine learning model. Performance metrics like classiffication accuracy and root mean squared error can give you a clear objective idea of how good a set of predictions is, and in turn how good the model is that generated them. This is important as it allows you to tell the difference and select among:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Different transforms of the data used to train the same machine learning model.</li>\n",
    "    <li>Different machine learning models trained on the same data.</li>\n",
    "    <li>Different configurations for a machine learning model trained on the same data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification accuracy\n",
    "\n",
    "<p style=\"text-align: justify\">A quick way to evaluate a set of predictions on a classification problem is by using accuracy.\n",
    "Classification accuracy is a ratio of the number of correct predictions out of all predictions that\n",
    "were made. It is often presented as a percentage between 0% for the worst possible accuracy\n",
    "and 100% for the best possible accuracy.</p>\n",
    "\n",
    "$$accuracy = \\dfrac{number \\ of \\ correct \\ predictions}{total \\ number \\ of \\ predictions} \\times 100 \\quad (\\%)$$\n",
    "\n",
    "<p style=\"text-align: justify\">We can implement this in a function that takes the expected outcomes and the predictions as arguments. Below is this function named <em>getAccuracy()</em> that returns classification accuracy as a percentage. Notice that we use == to compare the equality actual to predicted values. This allows us to compare integers or strings, two main data types that we may choose to use when loading classification data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of prediction #\n",
    "def getAccuracy(actual,predicted):\n",
    "    correct = 0\n",
    "    for x in range(len(actual)):\n",
    "        if actual[x] == predicted[x]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the function, we create 2 datasets of actual and predicted integer values and calculate the accuracy of the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy prediction : 80.0 %\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "accuracy = getAccuracy(actual, predicted)\n",
    "print(\"Accuracy prediction :\", accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Accuracy is a good metric to use when you have a small number of class values, such as 2,\n",
    "also called a binary classification problem. Accuracy starts to lose it's meaning when you have more class values and you may need to review a different perspective on the results, such as a confusion matrix.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">A confusion matrix provides a summary of all of the predictions made compared to the expected actual values. The results are presented in a matrix with counts in each cell. The counts of actual class values are summarized horizontally, whereas the counts of predictions for each class values are presented vertically. A perfect set of predictions is shown as a diagonal line from the top left to the bottom right of the matrix.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The value of a confusion matrix for classification problems is that you can clearly see which predictions were wrong and the type of mistake that was made. Let's create a function to calculate a confusion matrix.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can start off by defining the function to calculate the confusion matrix given a list of actual class values and a list of predictions. The function is listed below and is named <em>confusion matrix()</em>. It first makes a list of all of the unique class values and assigns each class value a unique integer or index into the confusion matrix.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The confusion matrix is always square, with the number of class values indicating the number of rows and columns required. Here, the first index into the matrix is the row for actual values and the second is the column for predicted values. After the square confusion matrix is created and initialized to zero counts in each cell, it is a matter of looping through all predictions and incrementing the count in each cell. The function returns two objects. The first is the set of unique class values, so that they can be displayed when the confusion matrix is drawn. The second is the confusion matrix itself with the counts in each cell.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The function <em>print_confusion_matrix</em> prints the confusion matrix for better display. It names the columns as P for Predictions and the rows as A for Actual. Each column and row are named for the class value to which it corresponds.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Confusion Matrix #\n",
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set(actual) # get unique values from actual matrix\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))] # ini confusion matrix with  0 everywhere\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique): # loop to lookup values in unique set\n",
    "        lookup[value] = i\n",
    "    for i in range(len(actual)): # loop to add the count of the values to each cell of the confusion matrix\n",
    "        x = lookup[actual[i]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[x][y] += 1\n",
    "    return unique, matrix\n",
    "\n",
    "# Printing a confusion matrix\n",
    "def print_confusion_matrix(unique, matrix):\n",
    "    print('Unique prediction values:')\n",
    "    print('(P)' + ' '.join(str(x) for x in unique))\n",
    "    print('(A)---')\n",
    "    print(\"Confusion Matrix:\")\n",
    "    for i, x in enumerate(unique):\n",
    "        print(\"%s| %s\" % (x, ' '.join(str(x) for x in matrix[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Running the example produces the output below. We can see the class labels of 0 and 1 across the top and bottom. Looking down the diagonal of the matrix from the top left to bottom right, we can see that 4 predictions of 0 were correct and 4 predictions of 1 were correct. Looking in the other cells, we can see 1 + 1 or 2 prediction errors. We can see that 1 prediction was made as a 1 that was in fact actually a 0 class value (top right corner). And we can see 1 prediction that was a 0 that was in fact actually a 1 (bottom left corner).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique prediction values:\n",
      "(P)0 1\n",
      "(A)---\n",
      "Confusion Matrix:\n",
      "0| 4 1\n",
      "1| 1 4\n"
     ]
    }
   ],
   "source": [
    "# Test confusion matrix with integers\n",
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print_confusion_matrix(unique, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and F1 score\n",
    "\n",
    "\n",
    "To be more specific about the confusion matrix, and the different accuracy measures that can be derived from it, we need a represenatation of the matrix, with a meaning of the different columns\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/935/1*g5zpskPaxO8uSl0OWT4NTQ.png\">\n",
    "\n",
    "<ul>\n",
    "    \n",
    "<li><b>True Positive (TP):</b></li>\n",
    "\n",
    "Interpretation: You predicted positive and it’s true. \n",
    "\n",
    "<br>\n",
    "\n",
    "<li><b>True Negative (TN):</b></li>\n",
    "\n",
    "Interpretation: You predicted negative and it’s true.\n",
    "\n",
    "<br>\n",
    "\n",
    "<li><b>False Positive (FP): (Type 1 Error)</b></li>\n",
    "\n",
    "Interpretation: You predicted positive and it’s false.\n",
    "\n",
    "<br>\n",
    "\n",
    "<li><b>False Negative (FN): (Type 2 Error)</b></li>\n",
    "\n",
    "Interpretation: You predicted negative and it’s false.\n",
    "\n",
    "Just remember that we describe predicted values as Positive and Negative and actual values as True and False.\n",
    "\n",
    "<br>\n",
    "\n",
    "<p style=\"text-align: justify\"><b>Precision</b> is the ratio of correctly predicted positive observations to the total predicted positive observations.</p>\n",
    "\n",
    "$$Precision = \\dfrac{TP}{TP + FP}$$\n",
    "\n",
    "<p style=\"text-align: justify\"><b>Recall (Sensitivity)</b> - Recall is the ratio of correctly predicted positive observations to to all observations in the actual class.</p>\n",
    "\n",
    "$$Recall = \\dfrac{TP}{TP + FN}$$\n",
    "\n",
    "<p style=\"text-align: justify\"><b>F1 score</b> - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall.</p>\n",
    "\n",
    "$$F1_{score} = 2 \\times \\dfrac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "\n",
    "<p style=\"text-align: justify\">The function <em>recall_precision_calc()</em> defined below takes the confusion matrix as argument to calculate both Precision,Recall and F1 score accuracy metrics. TP, FN and FP are first calculated by realizing operations on the confusion matrix. The Recall, Precision and F1 score metrics are then calculated by the formulas above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall classification estimator \n",
    "def recall_precision_calc(matrix):\n",
    "    for i in range(len(matrix[0])):\n",
    "        row_values = matrix[i] # row values of matrix\n",
    "        col_values = [row[i] for row in matrix] # column values of matrix\n",
    "        tp = col_values[i]\n",
    "        fp = sum(row_values)-row_values[i] # sum all row values - ones in diagonal\n",
    "        fn = sum(col_values)-col_values[i] # sum all col values - ones in diagonal\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return recall, precision, F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The same example as before gives the evaluation for all the metrics. In this particular example, as Recall and Precision are the same, F1 score is also the same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall : 0.8 ; Precision : 0.8 ; F1 score : 0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "recall, precision, F1_score = recall_precision_calc(matrix)\n",
    "\n",
    "print(\"Recall :\", recall, \"; Precision :\", precision, \"; F1 score :\", F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error\n",
    "\n",
    "<p style=\"text-align: justify\">Regression problems are those where a real value is predicted. An easy metric to consider is the error in the predicted values as compared to the expected values. The Mean Absolute Error or MAE for short is a good first error metric to use. It is calculated as the average of the absolute error values, where absolute means made positive so that they can be added together.</p>\n",
    "\n",
    "$$MAE = \\dfrac{\\sum_{i=1}^{n} |predicted_{i} - actual_{i}|}{total \\ number \\ of \\ predictions} = \\dfrac{1}{n} \\times \\sum_{i=1}^{n} |\\hat{y_i} - y_i| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Below is a function named <em>mae metric()</em> that implements this metric. As above, it expects a list of actual outcome values and a list of predictions. We use the built-in <em>abs()</em> Python function to calculate the absolute error values that are summed together.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute error (MAE) #\n",
    "def mae_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_error += abs(predicted[i] - actual[i])\n",
    "    return sum_error / float(len(actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The example below, gives a MAE of 0.016, so 1.6% relative difference between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015714285714285715\n"
     ]
    }
   ],
   "source": [
    "# Test RMSE\n",
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "predicted = [0.12, 0.18, 0.29, 0.42, 0.5, 0.63, 0.69]\n",
    "mae = mae_metric(actual, predicted)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error\n",
    "\n",
    "<p style=\"text-align: justify\">Root Mean Squared Error (RMSE) is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation.</p>\n",
    "\n",
    "$$RMSE = \\sqrt{\\dfrac{\\sum_{i=1}^{n} (predicted_{i} - actual_{i})^2}{total \\ number \\ of \\ predictions}} = \\sqrt{\\dfrac{1}{n} \\times \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2}$$\n",
    "\n",
    "<p style=\"text-align: justify\">Below is an implementation of this in a function named <em>rmse_metric()</em>. It uses the <em>sqrt()</em> function from the math module and uses the ** operator to raise the error to the 2nd power.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error #\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can test this metric on the same dataset used to test the calculation of Mean Absolute\n",
    "Error above. The expected error is around the order of the MAE, and indeed we find an error of 0.018 (1.8%).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018126539343499316\n"
     ]
    }
   ],
   "source": [
    "# Test RMSE\n",
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "predicted = [0.12, 0.18, 0.29, 0.42, 0.5, 0.63, 0.69]\n",
    "rmse = rmse_metric(actual, predicted)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "<p style=\"text-align: justify\">There exist many more accuracy metrics that can be implemented, such as:\n",
    "\n",
    "<ul>\n",
    "    <li>Area Under ROC Curve or AUC for classification.</li>\n",
    "    <li>Goodness of Fit or $R^2$ (R squared) for regression.</li>\n",
    "    <li>Logarithmic Loss</li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
